{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Respuestas_Tarea_1_Partes_1_2_CC6402_2018.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"iCszeuRk0NuH","colab_type":"text"},"cell_type":"markdown","source":["# Tarea 1 <br/> CC6204 Deep Learning, Universidad de Chile  <br/> Hoja de respuestas partes 1, 2, 3, 4 y 5\n","## Nombre: Martin Cornejo Saavedra\n","Fecha sugerida para completar esta parte: 23 de marzo de 2018"]},{"metadata":{"id":"ac_XrA2QDGYt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# instalacion de los paquetes necesarios\n","\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n","!pip install -q ipdb\n","\n","import os\n","import glob\n","import torch\n","import numpy\n","import pdb\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Uq9u0IfT0VRp","colab_type":"text"},"cell_type":"markdown","source":["# Parte 1: Funciones de activación, derivadas y función de salida"]},{"metadata":{"id":"DMw80P8o0qrJ","colab_type":"text"},"cell_type":"markdown","source":["## 1a) Funciones de activación"]},{"metadata":{"id":"tDhcNbNT0YNr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def relu(T):\n","    T[T < 0] = 0\n","    return T\n","\n","def sig(T):\n","    return torch.reciprocal(1 + torch.exp(-1 * T))\n","\n","def swish(T, beta):\n","    return torch.mul(T, sig(torch.mul(T, beta)))\n","\n","def celu(T, alfa):\n","    positive = relu(T)\n","    negative = torch.mul(relu(torch.mul(T, -1)), -1)\n","    celu_T = torch.mul(torch.add(torch.exp(torch.div(negative, alfa)), -1), alfa)\n","\n","    return torch.add(positive, 1, celu_T)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S_StkJsV07L3","colab_type":"text"},"cell_type":"markdown","source":["## 1b) Derivando las funciones de activación"]},{"metadata":{"id":"UXyOWjYi1Do8","colab_type":"text"},"cell_type":"markdown","source":["\n","\\begin{equation}\n","\\frac{\\partial\\ \\text{relu}(x)}{\\partial x} =\n","\\left\\{\n","\t\\begin{array}{ll}\n","\t\t1  & \\mbox{si } x \\geq 0 \\\\\n","\t\t0  & \\mbox{~} \n","\t\\end{array}\n","\\right. \n","\\end{equation}\n","<br>\n","\n","Dado $ \\sigma (x) = sigmoid(x)$, tenemos que:\n","\n","\\begin{eqnarray}\n","\\frac{\\partial\\ \\text{swish}(x, \\beta)}{\\partial x} & = \\sigma (\\beta x) + \\beta x \\cdot \\sigma (\\beta x)(1-\\sigma (\\beta x)) \\\\\n","& = \\sigma (\\beta x) + \\beta x \\cdot \\sigma (\\beta x) - \\beta x \\cdot \\sigma (\\beta x)^{2}  \\\\\n","&= \\beta \\cdot swish(x, \\beta) + \\sigma (\\beta x)(1 - \\beta \\cdot swish(x, \\beta))\\\\\n","\\\\\n","\\frac{\\partial\\ \\text{swish}(x, \\beta)}{\\partial \\beta} & =  \n","x^2 \\sigma (\\beta x)(1 - \\sigma (\\beta x))\\\\\n","\\end{eqnarray}\n","<br><br>\n","\n","\\begin{eqnarray}\n","\\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial x} & =  \n","\\left\\{\n","\t\\begin{array}{ll}\n","\t\t1  & \\mbox{si } x \\geq 0 \\\\\n","\t\texp (\\frac{x}{\\alpha})  & \\mbox{~} \n","\t\\end{array}\n","\\right. \\\\\n","\\\\\n","\\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial \\alpha} & = \n","\\left\\{\n","\t\\begin{array}{ll}\n","\t\t0  & \\mbox{si } x \\geq 0 \\\\\n","\t\texp (\\frac{x}{\\alpha})(1 - \\frac{x}{\\alpha}) - 1  & \\mbox{~} \n","\t\\end{array}\n","\\right. \\\\\n","\\end{eqnarray}"]},{"metadata":{"id":"e_0dTh7l1bas","colab_type":"text"},"cell_type":"markdown","source":["## 1c) Softmax\n","\n","Dada la funcion `softmax` sabemos que cada elemento de la secuencia $\\text{softmax}(x_1,\\ldots,x_n)$ tiene la forma\n","\n","\\begin{equation}\n","s_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n","\\end{equation}\n","\n","Luego, para cada elemento de la secuencia $\\text{softmax}(x_1-M,\\ldots,x_n-M)$ se tiene\n","\n","\\begin{equation}\n","s_i = \\frac{e^{x_i-M}}{\\sum_{j=1}^{n}e^{x_j-M}} = \\frac{e^{-M}e^{x_i}}{\\sum_{j=1}^{n}e^{-M}e^{x_j}} = \\frac{e^{-M}e^{x_i}}{e^{-M}\\sum_{j=1}^{n}e^{x_j}} = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n","\\end{equation}\n","\n","Demostrando que $\\text{softmax}(x_1-M,\\ldots,x_n-M) = \\text{softmax}(x_1,\\ldots,x_n)$."]},{"metadata":{"id":"jDg2sU7D1dIY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# por ahora softmax estara implementada solo para tensores en 2-D\n","def softmax(T, dim=0, estable=True):\n","    denom_softmax = torch.div(T, 2)\n","    denom_softmax = torch.exp(denom_softmax)\n","    denom_softmax = torch.mm(denom_softmax, torch.transpose(denom_softmax, 0, 1))\n","    denom_softmax = torch.reciprocal(torch.diag(denom_softmax))\n","\n","    return torch.mm(torch.diag(denom_softmax), T.exp())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"662XLsDA9XXI","colab_type":"text"},"cell_type":"markdown","source":["# Parte 2: Red neuronal y pasada hacia adelante (forward)"]},{"metadata":{"id":"fTUm9ZbX9bRA","colab_type":"text"},"cell_type":"markdown","source":["## 2a) Clase para red neuronal, 2b) Usando la GPU, 2c) Pasada hacia adelante"]},{"metadata":{"id":"f_jeuYbv9WhK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class FFNN():\n","    def __init__(self, F, l_h, l_a, C, params=[]):\n","      if (len(params) > 0):\n","        self.W_1 = params[0][0]\n","        self.b_1 = params[0][1]\n","        self.W_2 = params[1][0]\n","        self.b_2 = params[1][1]\n","        self.U = params[2][0]\n","        self.c_init = params[2][1]\n","\n","      else:\n","        self.F = F\n","        self.l_h = l_h\n","        self.l_a = l_a\n","        self.C = C\n","\n","        self.W_1 = torch.randn(F, l_h[0])\n","        self.b_1 = torch.zeros(1, l_h[0])\n","\n","        self.W_2 = torch.randn(l_h[0], l_h[1])\n","        self.b_2 = torch.zeros(1, l_h[1])\n","\n","        self.U = torch.randn(l_h[1], C)\n","        self.c_init = torch.zeros(1, C)\n","          \n","  \n","    def gpu(self):\n","      if torch.cuda.is_available():\n","        self.W_1 = self.W_1.cuda()\n","        self.b_1 = self.b_1.cuda()\n","        self.W_2 = self.W_2.cuda()\n","        self.b_2 = self.b_2.cuda()\n","        self.U = self.U.cuda()\n","        self.c_init = self.c_init.cuda()\n","  \n","    def cpu(self):\n","      self.W_1 = self.W_1.cpu()\n","      self.b_1 = self.b_1.cpu()\n","      self.W_2 = self.W_2.cpu()\n","      self.b_2 = self.b_2.cpu()\n","      self.U = self.U.cpu()\n","      self.c_init = self.c_init.cpu()\n","  \n","    def forward(self, x):\n","      if torch.cuda.is_available():\n","        x = x.cuda()\n","        self.gpu()   # redundante, corregir\n","      \n","      h_1 = sig(torch.mm(x, self.W_1) + self.b_1)\n","      h_2 = sig(torch.mm(h_1, self.W_2) + self.b_2)\n","      y = softmax(torch.mm(h_2, self.U) + self.c_init)\n","\n","      return y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bgf5Xx-34Pa1","colab_type":"text"},"cell_type":"markdown","source":["## 2d) Probando tu red con un modelo pre-entrenado"]},{"metadata":{"id":"twyLtUUnDok7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":123},"outputId":"606b8e20-43fc-49f7-d954-baabaa04fcdc","executionInfo":{"status":"ok","timestamp":1523214991665,"user_tz":180,"elapsed":2843,"user":{"displayName":"Martín Cornejo-Saavedra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100137397923643336617"}}},"cell_type":"code","source":["## Clonamos el github\n","\n","!git clone https://github.com/jorgeperezrojas/cc6204-DeepLearning-DCCUChile.git\n","\n","os.chdir(\"/content/cc6204-DeepLearning-DCCUChile/2018/tareas/tarea1/recursos/varita_magica\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Cloning into 'cc6204-DeepLearning-DCCUChile'...\n","remote: Counting objects: 5633, done.\u001b[K\n","remote: Compressing objects: 100% (2894/2894), done.\u001b[K\n","remote: Total 5633 (delta 2729), reused 5577 (delta 2705), pack-reused 0\u001b[K\n","Receiving objects: 100% (5633/5633), 5.48 MiB | 31.72 MiB/s, done.\n","Resolving deltas: 100% (2729/2729), done.\n"],"name":"stdout"}]},{"metadata":{"id":"dJUAEgvVQZ1Q","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":158},"outputId":"09e8b7b4-1b1d-4817-86de-1be854760288","executionInfo":{"status":"ok","timestamp":1523215024442,"user_tz":180,"elapsed":654,"user":{"displayName":"Martín Cornejo-Saavedra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100137397923643336617"}}},"cell_type":"code","source":["# cargar parametros entrenados a tensores\n","\n","local_download_path = \"modelos/ejemplo\"\n","params = []\n","params.append([numpy.loadtxt(local_download_path+\"/W1.txt\"), numpy.loadtxt(local_download_path+\"/b1.txt\")])\n","params.append([numpy.loadtxt(local_download_path+\"/W2.txt\"), numpy.loadtxt(local_download_path+\"/b2.txt\")])\n","params.append([numpy.loadtxt(local_download_path+\"/U.txt\"), numpy.loadtxt(local_download_path+\"/c.txt\")])\n","\n","params = list(map(lambda x: [torch.from_numpy(x[0]), torch.from_numpy(x[1])], params))\n","#pdb.set_trace()\n","\n","\n","# cargar red neuronal en pase a parametros conocidos\n","\n","test_data_path = \"data/train_set\"\n","test_input = torch.from_numpy(numpy.loadtxt(test_data_path+\"/hechizo-8/090.txt\")).double().cuda().view(1, 4096)\n","red_neuronal = FFNN(0, [], ['algo'], 10, params)\n","red_neuronal.forward(test_input)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","\n","Columns 0 to 5 \n"," 1.4113e-04  3.3992e-07  2.2298e-04  6.5426e-12  2.3017e-05  4.3196e-08\n","\n","Columns 6 to 9 \n"," 3.4144e-06  4.4418e-07  9.9961e-01  5.5327e-09\n","[torch.cuda.DoubleTensor of size 1x10 (GPU 0)]"]},"metadata":{"tags":[]},"execution_count":10}]},{"metadata":{"id":"SLeq3y8FE3SU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Tu código visualizando los ejemplos incorrectos acá"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9PSoJf8z5-Bp","colab_type":"text"},"cell_type":"markdown","source":["# Parte 3: Más derivadas y back propagation"]},{"metadata":{"id":"s93JeYT05-Bq","colab_type":"text"},"cell_type":"markdown","source":["## 3a) Entropía Cruzada"]},{"metadata":{"id":"MsPKTlei5-Bq","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def cross_ent_loss(Q,P):\n","    dimension = 0\n","    q_log = torch.log(Q)\n","    product = torch.mul(P, torch.reciprocal(q_log))\n","\n","    return torch.sum(product)/Q.size(dimension)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f8SZPzBz5-Bt","colab_type":"text"},"cell_type":"markdown","source":["## 3b) Derivando la última capa"]},{"metadata":{"id":"1NKDSLGk5-Bu","colab_type":"text"},"cell_type":"markdown","source":["\\begin{equation}\n","\\frac{\\partial \\cal L}{\\partial u^{(L)}} = \\frac{\\partial \\cal L}{\\partial ŷ} \\cdot \\frac{\\partial ŷ}{\\partial u^{(L)}} \\\\\n","\\end{equation}"]},{"metadata":{"id":"xuBMxoXR5-Bu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["B = 5; C = 10\n","\n","y = torch.ones(B,C)\n","y_pred = torch.ones(B,C)\n","\n","# Acá tu trozo de código. \n","# Primero agregamos algunas variables dummy para chequear \n","# que al menos las dimensiones están correctas\n","\n","dimL = 15\n","\n","hL = torch.ones(B,dimL)\n","U = torch.ones(dimL,C)\n","c_bias = torch.ones(C)\n","\n","uL = hL.mm(U).add(c_bias)\n","\n","# Notamos que por regla de la cadena, \n","# dL_duL = dL_dypred * dypred_duL\n","# el primer termino de la derecha es la derivada\n","# de cross entropy \n","# el segundo termino de la derecha es la derivada de softmax\n","\n","dL_duL = torch.mul(torch.mul(y, torch.add(y_pred, -1)), 1/y_pred.size(0))\n","\n","# El gradiente debe coincidir en dimensiones con la variable\n","\n","assert dL_duL.size() == uL.size()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"P5ATR2G75-By","colab_type":"text"},"cell_type":"markdown","source":["## 3b) Derivando la última capa (continuación)"]},{"metadata":{"id":"E_tFjoNn5-B0","colab_type":"text"},"cell_type":"markdown","source":["Usando la notación de Einstein tenemos que:\n","\n","\\begin{eqnarray}\n","(\\frac{\\partial\\cal L}{\\partial U})_{ij} &= \\frac{\\partial\\cal L}{\\partial u^{(L)}_{kl}} \\frac{\\partial u^{(L)}_{kl}}{\\partial U_{ij}} \\\\\n","\\end{eqnarray}\n","\n","Luego,\n","\n","\\begin{equation}\n","\\frac{\\partial u^{(L)}_{kl}}{\\partial U_{ij}} = \\frac{\\partial (h^{(L)}_{kr}U_{rl} + c_{l})}{\\partial U_{ij}} = \\left\\{\n","    \\begin{array}{}\n","\t\th^{(L)}_{ki}  & \\mbox{si } r = i,\\ l = j \\\\\n","\t\t0  & \\mbox{~}\n","    \\end{array}\n","\\right.\n","\\end{equation}\n","\n","Entonces,\n","\n","\\begin{eqnarray}\n","(\\frac{\\partial\\cal L}{\\partial U})_{ij} &= \\frac{\\partial\\cal L}{\\partial u^{(L)}_{kl}} h^{(L)}_{ki} = \\frac{\\partial\\cal L}{\\partial u^{(L)}_{kj}} h^{(L)}_{ki} \\\\\n","\\\\\n","& \\boxed{ \\frac{\\partial\\cal L}{\\partial U} = (h^{(L)})^{T} \\frac{\\partial\\cal L}{\\partial u^{(L)}} }\n","\\end{eqnarray}\n","\n","Análogamente\n","\n","\\begin{equation}\n","\\frac{\\partial u^{(L)}_{kl}}{\\partial c_{i}} = \\frac{\\partial (h^{(L)}_{kr}U_{rl} + c_{l})}{\\partial c_{i}} = \\left\\{\n","    \\begin{array}{}\n","\t\t1  & \\mbox{si } l = j \\\\\n","\t\t0  & \\mbox{~}\n","    \\end{array}\n","\\right.\n","\\end{equation}\n","\n","\\begin{eqnarray}\n","(\\frac{\\partial\\cal L}{\\partial c})_{i} &= \\frac{\\partial\\cal L}{\\partial u^{(L)}_{ki}} \\cdot 1\n","\\end{eqnarray}\n","\n","\\begin{equation}\n","\\boxed{ \\frac{\\partial\\cal L}{\\partial c} = [1 \\ldots 1] \\frac{\\partial\\cal L}{\\partial u^{(L)}} }\n","\\end{equation}\n","\n","Donde $[1 \\ldots 1]$ es un vector de unos de largo correspondiente al numero de fila del resultado de $\\frac{\\partial\\cal L}{\\partial u^{(L)}}$ \n","\n","Finalmente,\n","\n","\\begin{equation}\n","\\frac{\\partial u^{(L)}_{kl}}{\\partial h^{(L)}_{ij}} = \\frac{\\partial (h^{(L)}_{kr}U_{rl} + c_{l})}{\\partial h^{(L)}_{ij}} = \\left\\{\n","    \\begin{array}{}\n","\t\tU_{jl}  & \\mbox{si } k = i,\\ r = j \\\\\n","\t\t0  & \\mbox{~}\n","    \\end{array}\n","\\right.\n","\\end{equation}\n","\n","\\begin{eqnarray}\n","(\\frac{\\partial\\cal L}{\\partial h^{(L)}})_{ij} &= \\frac{\\partial\\cal L}{\\partial u^{(L)}_{kl}} U_{jl} = \\frac{\\partial\\cal L}{\\partial u^{(L)}_{il}} U_{lj}^{T} \\\\\n","\\\\\n","& \\boxed{ \\frac{\\partial\\cal L}{\\partial h^{(L)}} = \\frac{\\partial\\cal L}{\\partial u^{(L)}} U^{T} }\n","\\end{eqnarray}"]},{"metadata":{"id":"L7iN-pjO5-B1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["dL_dU = torch.mm(torch.transpose(hL, 0, 1), dL_duL)\n","\n","dL_dc = torch.mm(torch.ones(1, dL_duL.size(0)), dL_duL)\n","\n","dL_dhL = torch.mm(dL_duL, torch.transpose(U, 0, 1))\n","\n","# El gradiente debe coincidir en dimensiones con las variables\n","\n","assert dL_dU.size() == U.size()\n","assert dL_dc.size(1) == c_bias.size(0)\n","assert dL_dhL.size() == hL.size()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pFp2e-sr5-B7","colab_type":"text"},"cell_type":"markdown","source":["## 3c) Derivando desde las capas escondidas"]},{"metadata":{"id":"EvkTqmN15-B7","colab_type":"text"},"cell_type":"markdown","source":["Repite los siguientes cálculos para $\\text{relu}, \\text{celu}, \\text{swish}$\n","<br><br>\n","\n","\\begin{equation}\n","\\frac{\\partial\\cal L}{\\partial u^{(k)}} = \\ldots \\\\\n","\\end{equation}\n","<br><br>\n","\n","\\begin{equation}\n","\\frac{\\partial\\cal L}{\\partial W^{(k)}} = \\ldots \\\\\n","\\end{equation}\n","<br><br>\n","\n","\\begin{equation}\n","\\frac{\\partial\\cal L}{\\partial b^{(k)}} = \\ldots \\\\\n","\\end{equation}\n","<br><br>\n","\n","\\begin{equation}\n","\\frac{\\partial\\cal L}{\\partial h^{(k-1)}} = \\ldots \\\\\n","\\end{equation}"]},{"metadata":{"id":"4Zi63qOT5-B9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Acá tu trozo de código. \n","# Primero agregamos algunas variables dummy para chequear \n","# que al menos las dimensiones están correctas\n","\n","dimk = 20\n","dimkm1 = 15\n","\n","hk = torch.ones(B,dimk)\n","Wk = torch.ones(dimk,dimkm1)\n","bk = torch.ones(dimkm1)\n","\n","uk = hk.mm(Wk).add(bk)\n","\n","dL_dhkm1 = torch.ones(B,dimkm1)\n","\n","# Ahora tu fórmula para el gradiente.\n","# Esto debes repetirlo para relu, celu, y swish\n","\n","# para sigmoid\n","dL_duk_sig = torch.mul(dL_dhkm1, torch.mul(hL, torch.mul(torch.add(hL, -1), -1)))\n","\n","# para relu\n","dL_duk_rel = None\n","\n","# para celu\n","dL_duk_celu = None\n","\n","# para swish\n","dL_duk_swish = None\n","\n","# se elige una funcion de activacion\n","dL_duk = dL_duk_sig\n","\n","# estas derivadas se calculan de la misma forma para cualquier funcion de activacion\n","\n","dL_dWk = torch.mm(torch.transpose(hk, 0, 1), dL_duk)\n","dL_dbk = torch.mm(torch.ones(1, dL_duk.size(0)), dL_duk)\n","dL_dhk = torch.mm(dL_duk, torch.transpose(Wk, 0, 1))\n","\n","# El gradiente debe coincidir en dimensiones con las variables\n","\n","assert dL_duk.size() == uk.size()\n","assert dL_dWk.size() == Wk.size()\n","assert dL_dbk.size(1) == bk.size(0)\n","assert dL_dhk.size() == hk.size()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WGiXt9nd5-B_","colab_type":"text"},"cell_type":"markdown","source":["# Parte 4: Backpropagation en nuestra red"]},{"metadata":{"id":"Bz1Eg9Q15-CA","colab_type":"text"},"cell_type":"markdown","source":["## 4a) Método `backward`"]},{"metadata":{"id":"DBPnt65T5-CC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class FFNN():\n","    def __init__(self, F, l_h, l_a, C, params=[]):\n","      if (len(params) > 0):\n","        self.W_1 = params[0][0]\n","        self.b_1 = params[0][1]\n","        self.W_2 = params[1][0]\n","        self.b_2 = params[1][1]\n","        self.U = params[2][0]\n","        self.c_init = params[2][1]\n","\n","      else:\n","        self.F = F\n","        self.l_h = l_h\n","        self.l_a = l_a\n","        self.C = C\n","\n","        self.W_1 = torch.randn(F, l_h[0])\n","        self.b_1 = torch.zeros(1, l_h[0])\n","\n","        self.W_2 = torch.randn(l_h[0], l_h[1])\n","        self.b_2 = torch.zeros(1, l_h[1])\n","\n","        self.U = torch.randn(l_h[1], C)\n","        self.c_init = torch.zeros(1, C)\n","          \n","  \n","    def gpu(self):\n","      if torch.cuda.is_available():\n","        self.W_1 = self.W_1.cuda()\n","        self.b_1 = self.b_1.cuda()\n","        self.W_2 = self.W_2.cuda()\n","        self.b_2 = self.b_2.cuda()\n","        self.U = self.U.cuda()\n","        self.c_init = self.c_init.cuda()\n","  \n","    def cpu(self):\n","      self.W_1 = self.W_1.cpu()\n","      self.b_1 = self.b_1.cpu()\n","      self.W_2 = self.W_2.cpu()\n","      self.b_2 = self.b_2.cpu()\n","      self.U = self.U.cpu()\n","      self.c_init = self.c_init.cpu()\n","  \n","    def forward(self, x):\n","      if torch.cuda.is_available():\n","        x = x.cuda()\n","        self.gpu()   # redundante, corregir\n","      \n","      self.h_1 = sig(torch.mm(x, self.W_1) + self.b_1)\n","      self.h_2 = sig(torch.mm(self.h_1, self.W_2) + self.b_2)\n","      y = softmax(torch.mm(self.h_2, self.U) + self.c_init)\n","\n","      return y\n","  \n","    def backward(self, x, y, y_pred):\n","      B = 1; C = self.c_init.size(0)\n","      \n","      gradientes = {}\n","      \n","      # gradientes capa de salida\n","      \n","      dimL = self.b_2.size(0)\n","      uL = self.h_2.mm(self.U).add(self.c_init)\n","      \n","      dL_duL = torch.mul(torch.mul(y, torch.add(y_pred, -1)), 1/y_pred.size(0))\n","      dL_dU = torch.mm(torch.transpose(self.h_2, 0, 1), dL_duL)\n","      dL_dc = torch.mm(torch.ones(1, dL_duL.size(0)).double().cuda(), dL_duL)\n","      dL_dh2 = torch.mm(dL_duL, torch.transpose(self.U, 0, 1))\n","      \n","      assert dL_duL.size() == uL.size()\n","      assert dL_dU.size() == self.U.size()\n","      assert dL_dc.size(1) == self.c_init.size(0)\n","      assert dL_dh2.size() == self.h_2.size()\n","\n","      # gradientes segunda capa escondida\n","      \n","      u2 = self.h_1.mm(self.W_2).add(self.b_2)  \n","\n","      # para sigmoid\n","      dL_du2_sig = torch.mul(dL_dh2, torch.mul(self.h_2, torch.mul(torch.add(self.h_2, -1), -1)))\n","  \n","      # para relu\n","      dL_duk_rel = None\n","\n","      # para celu\n","      dL_duk_celu = None\n","\n","      # para swish\n","      dL_duk_swish = None\n","\n","      # se elige una funcion de activacion\n","      dL_du2 = dL_du2_sig\n","\n","      dL_dW2 = torch.mm(torch.transpose(self.h_1, 0, 1), dL_du2)\n","      dL_db2 = torch.mm(torch.ones(1, dL_du2.size(0)).double().cuda(), dL_du2)\n","      dL_dh1 = torch.mm(dL_du2, torch.transpose(self.W_2, 0, 1))\n","\n","      assert dL_du2.size() == u2.size()\n","      assert dL_dW2.size() == self.W_2.size()\n","      assert dL_db2.size(1) == self.b_2.size(0)\n","      assert dL_dh1.size() == self.h_1.size()\n","      \n","      # gradientes primera capa escondida\n","      \n","      u1 = x.mm(self.W_1).add(self.b_1)  \n","\n","      # para sigmoid\n","      dL_du1_sig = torch.mul(dL_dh1, torch.mul(self.h_1, torch.mul(torch.add(self.h_1, -1), -1)))\n","  \n","      # para relu\n","      dL_duk_rel = None\n","\n","      # para celu\n","      dL_duk_celu = None\n","\n","      # para swish\n","      dL_duk_swish = None\n","\n","      dL_du1 = dL_du1_sig\n","\n","      dL_dW1 = torch.mm(torch.transpose(x, 0, 1), dL_du1)\n","      dL_db1 = torch.mm(torch.ones(1, dL_du1.size(0)).double().cuda(), dL_du1)\n","      dL_dx = torch.mm(dL_du1, torch.transpose(self.W_1, 0, 1))\n","\n","      assert dL_du1.size() == u1.size()\n","      assert dL_dW1.size() == self.W_1.size()\n","      assert dL_db1.size(1) == self.b_1.size(0)\n","      assert dL_dx.size() == x.size()\n","      \n","      for gradiente in ['dL_db1', 'dL_dW1', 'dL_db2', 'dL_dW2', 'dL_dc', 'dL_dU']:\n","        gradientes[gradiente] = eval(gradiente)\n","      \n","      #pdb.set_trace()\n","      self.gradientes = gradientes\n","    \n","    def actualizarParams(self, lr):\n","      self.W_1 = self.W_1 - lr * self.gradientes['dL_dW1']\n","      self.b_1 = self.b_1 - lr * self.gradientes['dL_db1']\n","\n","      self.W_2 = self.W_2 - lr * self.gradientes['dL_dW2']\n","      self.b_2 = self.b_2 - lr * self.gradientes['dL_db2']\n","\n","      self.U = self.U - lr * self.gradientes['dL_dU']\n","      self.c_init = self.c_init - lr * self.gradientes['dL_dc']"],"execution_count":0,"outputs":[]},{"metadata":{"id":"njIqFC7LSDpz","colab_type":"text"},"cell_type":"markdown","source":["## 4b) Checkeo de gradiente"]},{"metadata":{"id":"qK_gEjYSSI5E","colab_type":"text"},"cell_type":"markdown","source":["# Parte 5: Descenso de gradiente y entrenamiento"]},{"metadata":{"id":"VI-zZtO7SmD2","colab_type":"text"},"cell_type":"markdown","source":["## 5a) Descenso de gradiente (estocástico)"]},{"metadata":{"id":"uAyl1_s0SpUQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class SGD():\n","  def __init__(self, red, lr):\n","    self.red = red\n","    self.lr = lr\n","  \n","  def step(self):\n","    self.red.actualizarParams(self.lr)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8wPIY0yL5-CE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# probado descenso\n","\n","red_neuronal = FFNN(0, [], ['algo'], 10, params)\n","y_pred = red_neuronal.forward(test_input)\n","y = torch.from_numpy(numpy.array([0,0,0,0,0,0,0,0,0,1])).double().cuda().view(1, 10)\n","red_neuronal.backward(test_input, y, y_pred)\n","\n","optimizador = SGD(red_neuronal, 0.1) \n","optimizador.step()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jkmkfc50xhls","colab_type":"text"},"cell_type":"markdown","source":["## 5b) Datos para carga"]},{"metadata":{"id":"rjjQYqqWxiYX","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class RandomDataset():\n","  def __init__(self, N, F, C):\n","    x = torch.rand(N, F)\n","    self.x = torch.bernoulli(x)\n","\n","    y = torch.rand(N,C)\n","    self.y = torch.bernoulli(y)\n","    \n","    self.large = N\n","  \n","  def __len__(self):\n","    return self.large\n","  \n","  def __getitem__(self, i):\n","    return (self.x[i,:], self.y[i,:])\n","  \n","  def paquetes(self, B):\n","    n_iters = int(self.x.size(0)/B)\n","    \n","    paquetes = []\n","    \n","    #pdb.set_trace()\n","    \n","    for index in range(n_iters):\n","      paquetes.append(self.elige_batch(self.x,self.y,B))\n","    \n","    #pdb.set_trace()\n","    \n","    return paquetes\n","    \n","  # Para elegir el siguiente batch (uno al azar) desde los datos de entrada\n","  def elige_batch(self, X, Y, b):\n","    N = X.size()[0]\n","    x_lista = []\n","    y_lista = []\n","  \n","    #  i = np.random.randint(N-b) # <-- descomentar esto para ejemplos\n","    for _ in range(b):\n","      i = numpy.random.randint(N) # <-- comentar esto para ejemplos\n","      x_lista.append(X[i:i+1])\n","      y_lista.append(Y[i:i+1])\n","  \n","      x = torch.cat(x_lista, dim=0)\n","      y = torch.cat(y_lista, dim=0)\n","  \n","    return x,y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tA6ZXqkO5Xc2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":35},"outputId":"34ed3868-2ef1-4086-c7cd-64f0ea402ad1","executionInfo":{"status":"ok","timestamp":1523219114328,"user_tz":180,"elapsed":667,"user":{"displayName":"Martín Cornejo-Saavedra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100137397923643336617"}}},"cell_type":"code","source":["# probando RandomDataset\n","\n","dataset = RandomDataset(100,10,5)\n","for x,y in dataset.paquetes(10):\n","  #pdb.set_trace()\n","  continue\n","\n","True"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":59}]}]}