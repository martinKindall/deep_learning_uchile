{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Respuestas_Tarea_1_Partes_1_2_CC6402_2018.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "iCszeuRk0NuH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tarea 1 <br/> CC6204 Deep Learning, Universidad de Chile  <br/> Hoja de respuestas partes 1 y 2 \n",
        "## Nombre: Martin Cornejo Saavedra\n",
        "Fecha sugerida para completar esta parte: 23 de marzo de 2018"
      ]
    },
    {
      "metadata": {
        "id": "ac_XrA2QDGYt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# instalacion de los paquetes necesarios\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install -q ipdb\n",
        "\n",
        "import torch\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uq9u0IfT0VRp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 1: Funciones de activaci贸n, derivadas y funci贸n de salida"
      ]
    },
    {
      "metadata": {
        "id": "DMw80P8o0qrJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1a) Funciones de activaci贸n"
      ]
    },
    {
      "metadata": {
        "id": "tDhcNbNT0YNr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def relu(T):\n",
        "    T[T < 0] = 0\n",
        "    return T\n",
        "\n",
        "def sig(T):\n",
        "    return torch.reciprocal(1 + torch.exp(-1 * T))\n",
        "\n",
        "def swish(T, beta):\n",
        "    return torch.mul(T, sig(torch.mul(T, beta)))\n",
        "\n",
        "def celu(T, alfa):\n",
        "    positive = relu(T)\n",
        "    negative = torch.mul(relu(torch.mul(T, -1)), -1)\n",
        "    celu_T = torch.mul(torch.add(torch.exp(torch.div(negative, alfa)), -1), alfa)\n",
        "\n",
        "    return torch.add(positive, 1, celu_T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_StkJsV07L3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1b) Derivando las funciones de activaci贸n"
      ]
    },
    {
      "metadata": {
        "id": "UXyOWjYi1Do8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\ \\text{relu}(x)}{\\partial x} =\n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t1  & \\mbox{si } x \\geq 0 \\\\\n",
        "\t\t0  & \\mbox{~} \n",
        "\t\\end{array}\n",
        "\\right. \n",
        "\\end{equation}\n",
        "<br>\n",
        "\n",
        "Dado $ \\sigma (x) = sigmoid(x)$, tenemos que:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial\\ \\text{swish}(x, \\beta)}{\\partial x} & = \\sigma (\\beta x) + \\beta x \\cdot \\sigma (\\beta x)(1-\\sigma (\\beta x)) \\\\\n",
        "& = \\sigma (\\beta x) + \\beta x \\cdot \\sigma (\\beta x) - \\beta x \\cdot \\sigma (\\beta x)^{2}  \\\\\n",
        "&= \\beta \\cdot swish(x, \\beta) + \\sigma (\\beta x)(1 - \\beta \\cdot swish(x, \\beta))\\\\\n",
        "\\\\\n",
        "\\frac{\\partial\\ \\text{swish}(x, \\beta)}{\\partial \\beta} & =  \n",
        "x^2 \\sigma (\\beta x)(1 - \\sigma (\\beta x))\\\\\n",
        "\\end{eqnarray}\n",
        "<br><br>\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial x} & =  \n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t1  & \\mbox{si } x \\geq 0 \\\\\n",
        "\t\texp (\\frac{x}{\\alpha})  & \\mbox{~} \n",
        "\t\\end{array}\n",
        "\\right. \\\\\n",
        "\\\\\n",
        "\\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial \\alpha} & = \n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t0  & \\mbox{si } x \\geq 0 \\\\\n",
        "\t\texp (\\frac{x}{\\alpha})(1 - \\frac{x}{\\alpha}) - 1  & \\mbox{~} \n",
        "\t\\end{array}\n",
        "\\right. \\\\\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "metadata": {
        "id": "e_0dTh7l1bas",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1c) Softmax\n",
        "\n",
        "Dada la funcion `softmax` sabemos que cada elemento de la secuencia $\\text{softmax}(x_1,\\ldots,x_n)$ tiene la forma\n",
        "\n",
        "\\begin{equation}\n",
        "s_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n",
        "\\end{equation}\n",
        "\n",
        "Luego, para cada elemento de la secuencia $\\text{softmax}(x_1-M,\\ldots,x_n-M)$ se tiene\n",
        "\n",
        "\\begin{equation}\n",
        "s_i = \\frac{e^{x_i-M}}{\\sum_{j=1}^{n}e^{x_j-M}} = \\frac{e^{-M}e^{x_i}}{\\sum_{j=1}^{n}e^{-M}e^{x_j}} = \\frac{e^{-M}e^{x_i}}{e^{-M}\\sum_{j=1}^{n}e^{x_j}} = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n",
        "\\end{equation}\n",
        "\n",
        "Demostrando que $\\text{softmax}(x_1-M,\\ldots,x_n-M) = \\text{softmax}(x_1,\\ldots,x_n)$."
      ]
    },
    {
      "metadata": {
        "id": "jDg2sU7D1dIY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# por ahora softmax estara implementada solo para tensores en 2-D\n",
        "def softmax(T, dim=0, estable=True):\n",
        "    denom_softmax = torch.div(T, 2)\n",
        "    denom_softmax = torch.exp(denom_softmax)\n",
        "    denom_softmax = torch.mm(denom_softmax, torch.transpose(denom_softmax, 0, 1))\n",
        "    denom_softmax = torch.reciprocal(torch.diag(denom_softmax))\n",
        "\n",
        "    return torch.mm(torch.diag(denom_softmax), T.exp())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "662XLsDA9XXI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parte 2: Red neuronal y pasada hacia adelante (forward)"
      ]
    },
    {
      "metadata": {
        "id": "fTUm9ZbX9bRA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2a) Clase para red neuronal, 2b) Usando la GPU, 2c) Pasada hacia adelante"
      ]
    },
    {
      "metadata": {
        "id": "f_jeuYbv9WhK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class FFNN():\n",
        "    def __init__(self, F, l_h, l_a, C):\n",
        "        self.F = F\n",
        "        self.l_h = l_h\n",
        "        self.l_a = l_a\n",
        "        self.C = C\n",
        "\n",
        "        self.W_1 = torch.randn(F, l_h[0])\n",
        "        self.b_1 = torch.zeros(1, l_h[0])\n",
        "\n",
        "        self.W_2 = torch.randn(l_h[0], l_h[1])\n",
        "        self.b_2 = torch.zeros(1, l_h[1])\n",
        "\n",
        "        self.U = torch.randn(l_h[1], C)\n",
        "        self.c_init = torch.zeros(1, C)\n",
        "  \n",
        "    def gpu(self):\n",
        "        if torch.cuda.is_available():\n",
        "            self.W_1 = self.W_1.cuda()\n",
        "            self.b_1 = self.b_1.cuda()\n",
        "            self.W_2 = self.W_2.cuda()\n",
        "            self.b_2 = self.b_2.cuda()\n",
        "            self.U = self.U.cuda()\n",
        "            self.c_init = self.c_init.cuda()\n",
        "  \n",
        "    def cpu(self):\n",
        "        self.W_1 = self.W_1.cpu()\n",
        "        self.b_1 = self.b_1.cpu()\n",
        "        self.W_2 = self.W_2.cpu()\n",
        "        self.b_2 = self.b_2.cpu()\n",
        "        self.U = self.U.cpu()\n",
        "        self.c_init = self.c_init.cpu()\n",
        "  \n",
        "    def forward(self, x):\n",
        "        if torch.cuda.is_available():\n",
        "          x = x.cuda()\n",
        "          self.gpu()   # redundante, corregir\n",
        "          \n",
        "        h_1 = sig(torch.mm(x, self.W_1) + self.b_1)\n",
        "        h_2 = sig(torch.mm(h_1, self.W_2) + self.b_2)\n",
        "        y = softmax(torch.mm(h_2, self.U) + self.c_init)\n",
        "        pdb.set_trace()\n",
        "\n",
        "        return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bgf5Xx-34Pa1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2d) Probando tu red con un modelo pre-entrenado"
      ]
    },
    {
      "metadata": {
        "id": "2zppplXd4QXa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 6
            },
            {
              "item_id": 7
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "adedffdb-4955-41ef-c13a-6f6f42b6c059",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1521768065316,
          "user_tz": 180,
          "elapsed": 13324,
          "user": {
            "displayName": "Mart铆n Cornejo-Saavedra",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100137397923643336617"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "red_neuronal = FFNN(4096, [15,15], ['algo'], 10)\n",
        "red_neuronal.forward(torch.randn(3,4096))\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> <ipython-input-19-aec09e6b2f20>(44)forward()\n",
            "-> return y\n",
            "(Pdb) h_1\n",
            "\n",
            "\n",
            "Columns 0 to 9 \n",
            " 0.0000  0.0000  0.0000  0.0000  0.0000  0.9144  1.0000  0.0000  1.0000  1.0000\n",
            " 1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  1.0000  1.0000  1.0000  0.0000\n",
            " 0.0000  1.0000  0.0000  0.0000  0.0000  0.0017  0.0000  1.0000  1.0000  0.0000\n",
            "\n",
            "Columns 10 to 14 \n",
            " 0.0135  1.0000  0.0000  0.0000  1.0000\n",
            " 0.0000  1.0000  0.0013  1.0000  0.0922\n",
            " 0.9446  1.0000  0.0000  1.0000  0.0082\n",
            "[torch.cuda.FloatTensor of size 3x15 (GPU 0)]\n",
            "\n",
            "(Pdb) x\n",
            "\n",
            "-1.1777e+00 -5.2743e-01  2.9300e-01  ...   6.5543e-01  2.2869e-01  9.0177e-01\n",
            "-3.5474e-01 -3.7789e-01  1.4922e+00  ...  -7.3015e-01 -8.3007e-02  7.3020e-01\n",
            "-4.5178e-02  2.0084e-01  7.3121e-01  ...  -7.3172e-02 -1.9171e-01  1.4281e+00\n",
            "[torch.cuda.FloatTensor of size 3x4096 (GPU 0)]\n",
            "\n",
            "(Pdb) continue\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              " 0.0102  0.0014  0.0104  0.0018  0.0036  0.0087  0.9449  0.0125  0.0038  0.0027\n",
              " 0.0040  0.0034  0.0011  0.0009  0.0415  0.0060  0.9343  0.0022  0.0065  0.0001\n",
              " 0.1244  0.0233  0.0176  0.0115  0.3440  0.0101  0.4422  0.0091  0.0170  0.0008\n",
              "[torch.cuda.FloatTensor of size 3x10 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "SLeq3y8FE3SU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Tu c贸digo visualizando los ejemplos incorrectos ac谩"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}