{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Respuestas_Tarea_1_Partes_1_2_CC6402_2018.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"iCszeuRk0NuH","colab_type":"text"},"cell_type":"markdown","source":["# Tarea 1 <br/> CC6204 Deep Learning, Universidad de Chile  <br/> Hoja de respuestas partes 1 y 2 \n","## Nombre: Martin Cornejo Saavedra\n","Fecha sugerida para completar esta parte: 23 de marzo de 2018"]},{"metadata":{"id":"ac_XrA2QDGYt","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# instalacion de los paquetes necesarios\n","\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n","!pip install -q ipdb\n","\n","import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Uq9u0IfT0VRp","colab_type":"text"},"cell_type":"markdown","source":["# Parte 1: Funciones de activación, derivadas y función de salida"]},{"metadata":{"id":"DMw80P8o0qrJ","colab_type":"text"},"cell_type":"markdown","source":["## 1a) Funciones de activación"]},{"metadata":{"id":"tDhcNbNT0YNr","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def relu(T):\n","    T[T < 0] = 0\n","    return T\n","\n","def sig(T):\n","    return torch.reciprocal(1 + torch.exp(-1 * T))\n","\n","def swish(T, beta):\n","    return torch.mul(T, sig(torch.mul(T, beta)))\n","\n","def celu(T, alfa):\n","    positive = relu(T)\n","    negative = torch.mul(relu(torch.mul(T, -1)), -1)\n","    celu_T = torch.mul(torch.add(torch.exp(torch.div(negative, alfa)), -1), alfa)\n","\n","    return torch.add(positive, 1, celu_T)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S_StkJsV07L3","colab_type":"text"},"cell_type":"markdown","source":["## 1b) Derivando las funciones de activación"]},{"metadata":{"id":"UXyOWjYi1Do8","colab_type":"text"},"cell_type":"markdown","source":["\n","\\begin{equation}\n","\\frac{\\partial\\ \\text{relu}(x)}{\\partial x} =\n","\\left\\{\n","\t\\begin{array}{ll}\n","\t\t1  & \\mbox{si } x \\geq 0 \\\\\n","\t\t0  & \\mbox{~} \n","\t\\end{array}\n","\\right. \n","\\end{equation}\n","<br>\n","\n","Dado $ \\sigma (x) = sigmoid(x)$, tenemos que:\n","\n","\\begin{eqnarray}\n","\\frac{\\partial\\ \\text{swish}(x, \\beta)}{\\partial x} & = \\sigma (\\beta x) + \\beta x \\cdot \\sigma (\\beta x)(1-\\sigma (\\beta x)) \\\\\n","& = \\sigma (\\beta x) + \\beta x \\cdot \\sigma (\\beta x) - \\beta x \\cdot \\sigma (\\beta x)^{2}  \\\\\n","&= \\beta \\cdot swish(x, \\beta) + \\sigma (\\beta x)(1 - \\beta \\cdot swish(x, \\beta))\\\\\n","\\\\\n","\\frac{\\partial\\ \\text{swish}(x, \\beta)}{\\partial \\beta} & =  \n","x^2 \\sigma (\\beta x)(1 - \\sigma (\\beta x))\\\\\n","\\end{eqnarray}\n","<br><br>\n","\n","\\begin{eqnarray}\n","\\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial x} & =  \n","\\left\\{\n","\t\\begin{array}{ll}\n","\t\t1  & \\mbox{si } x \\geq 0 \\\\\n","\t\texp (\\frac{x}{\\alpha})  & \\mbox{~} \n","\t\\end{array}\n","\\right. \\\\\n","\\\\\n","\\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial \\alpha} & = \n","\\left\\{\n","\t\\begin{array}{ll}\n","\t\t0  & \\mbox{si } x \\geq 0 \\\\\n","\t\texp (\\frac{x}{\\alpha})(1 - \\frac{x}{\\alpha}) - 1  & \\mbox{~} \n","\t\\end{array}\n","\\right. \\\\\n","\\end{eqnarray}"]},{"metadata":{"id":"e_0dTh7l1bas","colab_type":"text"},"cell_type":"markdown","source":["## 1c) Softmax\n","\n","Dada la funcion `softmax` sabemos que cada elemento de la secuencia $\\text{softmax}(x_1,\\ldots,x_n)$ tiene la forma\n","\n","\\begin{equation}\n","s_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n","\\end{equation}\n","\n","Luego, para cada elemento de la secuencia $\\text{softmax}(x_1-M,\\ldots,x_n-M)$ se tiene\n","\n","\\begin{equation}\n","s_i = \\frac{e^{x_i-M}}{\\sum_{j=1}^{n}e^{x_j-M}} = \\frac{e^{-M}e^{x_i}}{\\sum_{j=1}^{n}e^{-M}e^{x_j}} = \\frac{e^{-M}e^{x_i}}{e^{-M}\\sum_{j=1}^{n}e^{x_j}} = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n","\\end{equation}\n","\n","Demostrando que $\\text{softmax}(x_1-M,\\ldots,x_n-M) = \\text{softmax}(x_1,\\ldots,x_n)$."]},{"metadata":{"id":"jDg2sU7D1dIY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# por ahora softmax estara implementada solo para tensores en 2-D\n","def softmax(T, dim=0, estable=True):\n","    denom_softmax = torch.div(T, 2)\n","    denom_softmax = torch.exp(denom_softmax)\n","    denom_softmax = torch.mm(denom_softmax, torch.transpose(denom_softmax, 0, 1))\n","    denom_softmax = torch.reciprocal(torch.diag(denom_softmax))\n","\n","    return torch.mm(torch.diag(denom_softmax), T.exp())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"662XLsDA9XXI","colab_type":"text"},"cell_type":"markdown","source":["# Parte 2: Red neuronal y pasada hacia adelante (forward)"]},{"metadata":{"id":"fTUm9ZbX9bRA","colab_type":"text"},"cell_type":"markdown","source":["## 2a) Clase para red neuronal, 2b) Usando la GPU, 2c) Pasada hacia adelante"]},{"metadata":{"id":"f_jeuYbv9WhK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class FFNN():\n","    def __init__(self, F, l_h, l_a, C):\n","        self.F = F\n","        self.l_h = l_h\n","        self.l_a = l_a\n","        self.C = C\n","\n","        self.W_1 = torch.randn(F, l_h)\n","        self.b_1 = torch.zeros(1, l_h)\n","        self.U = torch.randn(l_h, C)\n","        self.c_init = torch.zeros(1, C)\n","  \n","    def gpu(self):\n","        if torch.cuda.is_available():\n","            self.W_1 = self.W_1.cuda()\n","            self.b_1 = self.b_1.cuda()\n","            self.U = self.U.cuda()\n","            self.c_init = self.c_init.cuda()\n","  \n","    def cpu(self):\n","        self.W_1 = self.W_1.cpu()\n","        self.b_1 = self.b_1.cpu()\n","        self.U = self.U.cpu()\n","        self.c_init = self.c_init.cpu()\n","  \n","    def forward(self, x):\n","        h_1 = sig(torch.mm(x, self.W_1) + self.b_1)\n","        y = softmax(torch.mm(h_1, self.U) + self.c_init)\n","        #pdb.set_trace()\n","\n","        return y"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bgf5Xx-34Pa1","colab_type":"text"},"cell_type":"markdown","source":["## 2d) Probando tu red con un modelo pre-entrenado"]},{"metadata":{"id":"2zppplXd4QXa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":101},"outputId":"63b77a23-f1fb-4821-fc3a-9e868b41ef7a","executionInfo":{"status":"ok","timestamp":1521745028595,"user_tz":180,"elapsed":605,"user":{"displayName":"Martín Cornejo-Saavedra","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100137397923643336617"}}},"cell_type":"code","source":["import pdb\n","\n","red_neuronal = FFNN(4, 4, ['algo'], 2)\n","red_neuronal.forward(torch.randn(3,4))"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n"," 0.3053  0.6947\n"," 0.2965  0.7035\n"," 0.2637  0.7363\n","[torch.FloatTensor of size 3x2]"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"SLeq3y8FE3SU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Tu código visualizando los ejemplos incorrectos acá"],"execution_count":0,"outputs":[]}]}