{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea_1_CC6204_2018_Martin_Cornejo",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1lgdchHUBlqrPYZkBYOH-HIXzoaSkb9hK",
          "timestamp": 1524320794057
        },
        {
          "file_id": "1-7Alg0DzdesaVPVuDghwoXbN0pa8ds1t",
          "timestamp": 1521245040538
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "4zhjpqvcdo5o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tarea 1 <br/> CC6204 Deep Learning, Universidad de Chile <br/> Hoja de respuestas \n",
        "\n",
        "## Nombre: Martín Cornejo Saavedra\n",
        "Fecha para completar la tarea: miercoles 25 de abril de 2018\n"
      ]
    },
    {
      "metadata": {
        "id": "C3RE-PBXwnxB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Insturcciones:\n",
        "En este notebook debes dejar todo tu código de las partes 1 hasta 8 de la tarea. Debes dejar el código (y todo lo adicional que hayas programado) en las celdas designadas para ello. Las partes que se hacen a mano (o con fórmulas), puedes entregarlas al final o en un archivo separado. En la siguiente celda encontrarás un *check list* de las partes de la tarea. En cada item indica si lo completaste o no en tu entrega ('SI', o 'NO'). Por favor, no marques como 'SI' partes que no hiciste. Adicionalemnte, el lugar en tu código donde completaste cada parte, márcala con un comentario como el siguiente\n",
        "\n",
        "```\n",
        "#### Parte 4a) Método backward\n",
        "```\n",
        "\n",
        "Si bien para algunas partes puede no estar exactamente claro dónde comienza (y termina) cada parte, usa tu criterio para los comentarios. Esto es sólo para ayudarnos a corregir. Nota que las celdas de más abajo no necesariamente siguen el orden del check list. Por ejemplo en la celda donde defines tu clase para la red neuronal se espera agregues código de las partes 2, 3, 7 etc. Todo eso está en una única celda (lo entenderás cuando mires más abajo).\n",
        "\n",
        "Por favor sigue este formato. **Si no sigues el formato te descontaremos puntaje.**"
      ]
    },
    {
      "metadata": {
        "id": "jRUVrysiyuZs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Checklist\n",
        "\n",
        "Parte|Completado\n",
        "---:|---:\n",
        "**1) Activación, derivadas, y salida** |\n",
        "1a) Funciones de activación | SI\n",
        "1b) Derivando las funciones de activación | SI\n",
        "1c) Softmax | SI\n",
        "**2) Red neuronal y pasada hacia adelante** |\n",
        "2a) Clase para red neuronal | SI\n",
        "2b) Usando la GPU | SI\n",
        "2c) Pasada hacia adelante | SI\n",
        "2d) Probando tu red con un modelo pre-entrenado | SI (falta accurracy)\n",
        "**3) Más derivadas y backpropagation** |\n",
        "3a) Entropía cruzada | SI\n",
        "3b) Derivando la última capa | SI (falta desarrollo de derivar softmax)\n",
        "3c) Derivando desde las capas escondidas | SI (falta derivar celu, relu, etc..)\n",
        "**4) Backpropagation en nuestra red** |\n",
        "4a) Método backward | SI\n",
        "4b) Checkeo de gradiente | NO\n",
        "4c) Opcional: incluyendo los parámetros de celu y swish | NO\n",
        "**5) Descenso de gradiente y entrenamiento** |\n",
        "5a) Descenso de gradiente | SI\n",
        "5b) Datos para carga | SI\n",
        "5c) Entrenando la red | SI\n",
        "5d) Graficando la pérdida en el tiempo | SI\n",
        "5e) Entrenando con datos no random | NO\n",
        "**6) Regularización** |\n",
        "6a) Regularización por penalización de norma | SI\n",
        "6b) Regularización por dropout | SI\n",
        "**7) Optimización** |\n",
        "7a) Inicialización de Xavier | NO\n",
        "7b) Descenso de gradiente con momentum | NO\n",
        "7c) RMSProp | NO\n",
        "7d) Adam | NO\n",
        "7e) Opcional: batch normalization | NO\n",
        "**8) Entrenando sobre MNIST** |\n",
        "8a) Cargando y visualizando datos de MNIST | NO\n",
        "8b) Red neuronal para MNIST | NO\n",
        "8c) Opcional: visualización de entrenamiento y convergencia | NO\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "I_kVL7undMBx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Este notebook está pensado para correr en CoLaboratory. \n",
        "# Comenzamos instalando las librerías necesarias.\n",
        "# Si lo estás ejecutando localmente posiblemente no sea necesario\n",
        "# reinstalar todo.\n",
        "\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "!pip install -q ipdb\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import numpy\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Agrega acá todo lo que quieras importar o instalar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_EhdkSIOFvR9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Funciones de activación, predicción y pérdida"
      ]
    },
    {
      "metadata": {
        "id": "p80-9lwaUAix",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sig(T):\n",
        "  return torch.reciprocal(1 + torch.exp(-1 * T))\n",
        "\n",
        "def tanh(T):\n",
        "  E = torch.exp(T)\n",
        "  e = torch.exp(-1 * T)\n",
        "  return (E - e) * torch.reciprocal(E + e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VtQWFePbtrF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 1a) Funciones de activación\n",
        "\n",
        "def relu(T):\n",
        "    T[T < 0] = 0\n",
        "    return T\n",
        "\n",
        "def swish(T, beta):\n",
        "    return torch.mul(T, sig(torch.mul(T, beta)))\n",
        "\n",
        "def celu(T, alfa):\n",
        "    positive = relu(T)\n",
        "    negative = torch.mul(relu(torch.mul(T, -1)), -1)\n",
        "    celu_T = torch.mul(torch.add(torch.exp(torch.div(negative, alfa)), -1), alfa)\n",
        "\n",
        "    return torch.add(positive, 1, celu_T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5hVvsHp-dGk4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "#### Parte 1c) Softmax está implementada solo para tensores en 2-D\n",
        "def softmax(T, dim=0, estable=True):\n",
        "    denom_softmax = torch.div(T, 2)\n",
        "    denom_softmax = torch.exp(denom_softmax)\n",
        "    denom_softmax = torch.mm(denom_softmax, torch.transpose(denom_softmax, 0, 1))\n",
        "    denom_softmax = torch.reciprocal(torch.diag(denom_softmax))\n",
        "\n",
        "    return torch.mm(torch.diag(denom_softmax), T.exp())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FKxJiOB7Dbl",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 3a) Entropía cruzada\n",
        "def cross_ent_loss(Q,P):\n",
        "    dimension = 0\n",
        "    q_log = torch.log(Q)\n",
        "    product = torch.mul(P, torch.reciprocal(q_log))\n",
        "\n",
        "    return -torch.sum(product)/Q.size(dimension)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2m3i_1ijPLr_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Clase FFNN, incialización, forward, backward y regularización\n"
      ]
    },
    {
      "metadata": {
        "id": "E79vHyuu4mIZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 2a) Clase para red neuronal\n",
        "class FFNN():\n",
        "  \n",
        "  def __init__(  \n",
        "    self, F, l_h, l_a, C, \n",
        "    params=[],\n",
        "    wc_par=None,\n",
        "    keep_prob=[]\n",
        "  ):\n",
        "      self.parametros = []\n",
        "      \n",
        "      if (len(params) > 0):    \n",
        "        for param in params:                \n",
        "          self.parametros.append((param[0], param[1].view(1, param[1].size(0))))\n",
        "\n",
        "      else:\n",
        "        self.F = F\n",
        "        l_h.append(C)\n",
        "        self.l_h = l_h        \n",
        "        self.C = C       \n",
        "\n",
        "        for idx, neuronas in enumerate(self.l_h):\n",
        "          if (idx == 0):\n",
        "            pesos = torch.randn(F, neuronas)\n",
        "          else:\n",
        "            pesos = torch.randn(self.l_h[idx-1], neuronas)\n",
        "\n",
        "          bias = torch.randn(1, neuronas)\n",
        "          self.parametros.append((pesos, bias))\n",
        "        \n",
        "      self.l_a = l_a\n",
        "      self.wc_par = wc_par        \n",
        "      self.keep_prob = keep_prob\n",
        "      self.nHidden = len(l_a)\n",
        "        \n",
        "        \n",
        "  #### Parte 2b) Usando la GPU\n",
        "  \n",
        "  def toDouble(self):\n",
        "    for idx, param in enumerate(self.parametros):     \n",
        "        self.parametros[idx] = (param[0].double(), param[1].double())\n",
        "  \n",
        "  def gpu(self):\n",
        "    if torch.cuda.is_available():\n",
        "      for idx, param in enumerate(self.parametros):\n",
        "        self.parametros[idx] = (param[0].cuda(), param[1].cuda())\n",
        "        \n",
        "      for idx, mascara in enumerate(self.mascaras):\n",
        "        self.mascaras[idx] = mascara.cuda()\n",
        "  \n",
        "  def cpu(self):\n",
        "    for idx, param in enumerate(self.parametros):\n",
        "        self.parametros[idx] = (param[0].cpu(), param[1].cpu())\n",
        "    \n",
        "    #### Parte 7a) Inicialización de Xavier\n",
        "    #### Parte 7e) Opcional: batch normalization\n",
        "    \n",
        "  \n",
        "  #### Parte 2c) Pasada hacia adelante\n",
        "  def forward(self, x, predict=False):\n",
        "    self.toDouble()\n",
        "    x = x.double()\n",
        "    \n",
        "    self.mascaras = []\n",
        "    \n",
        "    # Parte 6b) inverted dropout    \n",
        "    if (len(self.keep_prob) == 0 or predict):\n",
        "      self.keep_prob = torch.ones(self.nHidden+1)   # no se apagan neuronas\n",
        "    \n",
        "    mascaraParams = [(x,1)]\n",
        "    mascaraParams += self.parametros\n",
        "    \n",
        "    for idx, prob in enumerate(self.keep_prob):\n",
        "      weights = mascaraParams[idx][0]    \n",
        "      if (len(weights.size()) == 1):\n",
        "        weights = weights.view(1, weights.size(0))\n",
        "      mascara = (torch.rand(mascaraParams[0][0].size(0), weights.size(1)).double() < prob).double() * (1/prob)\n",
        "      self.mascaras.append(mascara)\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "      x = x.cuda()\n",
        "      self.gpu()  \n",
        "      \n",
        "    else:   \n",
        "      self.cpu()\n",
        "               \n",
        "      \n",
        "    self.h_array = []    \n",
        "    \n",
        "    for idx, f_activacion in enumerate(self.l_a):\n",
        "      if (idx == 0):\n",
        "        self.h_layer = f_activacion(torch.mm(x*self.mascaras[0], self.parametros[idx][0]) + self.parametros[idx][1]) * self.mascaras[idx+1] # Parte 6b) inverted dropout\n",
        "      else:\n",
        "        self.h_layer = f_activacion(torch.mm(self.h_layer, self.parametros[idx][0]) + self.parametros[idx][1]) * self.mascaras[idx+1] \n",
        "      \n",
        "      self.h_array.append(self.h_layer)\n",
        "\n",
        "    y = softmax(torch.mm(self.h_layer, self.parametros[len(self.parametros)-1][0]) + self.parametros[len(self.parametros)-1][1])\n",
        "    self.h_array.append(y)\n",
        "\n",
        "    return y\n",
        "\n",
        "  \n",
        "  #### Parte 4a) Método backward\n",
        "  def chooseDeriv(self, funcion_activacion):\n",
        "    if (funcion_activacion == sig):\n",
        "      return lambda x: torch.mul(x, torch.mul(torch.add(x, -1), -1))\n",
        "    else:\n",
        "      raise Exception('Derivada no implementada')\n",
        "  \n",
        "  def backward(self,x,y,y_pred):\n",
        "    nHidden = len(self.l_a)\n",
        "    gradientes = [0] * (nHidden + 1)\n",
        "    deriv_activaciones = [0] * (nHidden + 1)\n",
        "\n",
        "    # gradientes capa de salida\n",
        "          \n",
        "    #dL_duL = torch.mul(torch.mul(y, torch.add(y_pred, -1)), 1/y_pred.size(0))\n",
        "    dL_duL = torch.mul(torch.add(y_pred, torch.mul(y, -1)), 1/y_pred.size(0))\n",
        "    dL_dU = torch.mm(torch.transpose(self.h_array[nHidden-1], 0, 1), dL_duL)\n",
        "    dL_dc = torch.mm(torch.ones(1, dL_duL.size(0)).double().cuda(), dL_duL)\n",
        "    dL_dhL = torch.mm(dL_duL, torch.transpose(self.parametros[nHidden][0], 0, 1))\n",
        "      \n",
        "    assert dL_duL.size() == self.h_array[nHidden].size()\n",
        "    assert dL_dU.size() == self.parametros[nHidden][0].size()\n",
        "    assert dL_dc.size(1) == self.parametros[nHidden][1].size(1)\n",
        "    assert dL_dhL.size() == self.h_array[nHidden-1].size()\n",
        "\n",
        "    gradientes[nHidden] = (dL_dU, dL_dc)\n",
        "    deriv_activaciones[nHidden] = dL_dhL\n",
        "\n",
        "    # gradientes capas escondidas\n",
        "\n",
        "    for idx, f_activacion in enumerate(self.l_a):\n",
        "\n",
        "      derivadaActivacion = self.chooseDeriv(self.l_a[nHidden-idx-1])\n",
        "      \n",
        "      dL_du_hidden = torch.mul(deriv_activaciones[nHidden-idx], derivadaActivacion(self.h_array[nHidden-idx-1]))\n",
        "\n",
        "      if (nHidden-1 == idx):\n",
        "        dL_dW = torch.mm(torch.transpose(x, 0, 1), dL_du_hidden)\n",
        "      else:\n",
        "        dL_dW = torch.mm(torch.transpose(self.h_array[nHidden-idx-2], 0, 1), dL_du_hidden)\n",
        "\n",
        "      dL_db = torch.mm(torch.ones(1, dL_du_hidden.size(0)).double().cuda(), dL_du_hidden)\n",
        "      dL_dh_n_1 = torch.mm(dL_du_hidden, torch.transpose(self.parametros[nHidden-idx-1][0], 0, 1))\n",
        "\n",
        "      assert dL_du_hidden.size() == self.h_array[nHidden-idx-1].size()\n",
        "      assert dL_dW.size() == self.parametros[nHidden-idx-1][0].size()\n",
        "      assert dL_db.size(1) == self.parametros[nHidden-idx-1][1].size(1)\n",
        "\n",
        "      if (nHidden-2-idx > -1):   # no comparamos el tamaño de dL/dx, no es relevante\n",
        "        assert dL_dh_n_1.size() == self.h_array[nHidden-2-idx].size()\n",
        "\n",
        "      gradientes[nHidden-idx-1] = (dL_dW, dL_db)\n",
        "      deriv_activaciones[nHidden-idx-1] = dL_dh_n_1\n",
        "         \n",
        "    self.gradientes = gradientes\n",
        "      \n",
        "      \n",
        "  def actualizarParams(self, lr):\n",
        "    \n",
        "    # mover esto al optimizador, para luego poder calcular los nuevos gradientes usando\n",
        "    # wc_par   \n",
        "    \n",
        "    const = 1\n",
        "    \n",
        "    # Parte 6a) agregando weight decay\n",
        "    if (self.wc_par != None):\n",
        "      const = const - lr * self.wc_par\n",
        "    \n",
        "    for idx in range(len(self.l_a) + 1):\n",
        "      W_aux = self.parametros[idx][0] * const - lr * self.gradientes[idx][0]\n",
        "      b_aux = self.parametros[idx][1] * const - lr * self.gradientes[idx][1]\n",
        "\n",
        "      self.parametros[idx] = (W_aux, b_aux)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lkOpCOUj7a-X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Probando tu red con un modelo pre-entrenado y visualizando casos incorrectos"
      ]
    },
    {
      "metadata": {
        "id": "WdX6bv6NvTYR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "85d30757-7e99-4201-a650-d5734fbc525f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524363100094,
          "user_tz": 180,
          "elapsed": 728,
          "user": {
            "displayName": "Martín Cornejo-Saavedra",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100137397923643336617"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 2d) Probando tu red con un modelo pre-entrenado\n",
        "\n",
        "## Clonamos el github\n",
        "\n",
        "#!git clone https://github.com/jorgeperezrojas/cc6204-DeepLearning-DCCUChile.git\n",
        "\n",
        "os.chdir(\"/content/cc6204-DeepLearning-DCCUChile/2018/tareas/tarea1/recursos/varita_magica\")\n",
        "\n",
        "# cargar parametros entrenados a tensores\n",
        "\n",
        "local_download_path = \"modelos/ejemplo\"\n",
        "params = []\n",
        "params.append([numpy.loadtxt(local_download_path+\"/W1.txt\"), numpy.loadtxt(local_download_path+\"/b1.txt\")])\n",
        "params.append([numpy.loadtxt(local_download_path+\"/W2.txt\"), numpy.loadtxt(local_download_path+\"/b2.txt\")])\n",
        "params.append([numpy.loadtxt(local_download_path+\"/U.txt\"), numpy.loadtxt(local_download_path+\"/c.txt\")])\n",
        "\n",
        "params = list(map(lambda x: [torch.from_numpy(x[0]), torch.from_numpy(x[1])], params))\n",
        "#pdb.set_trace()\n",
        "\n",
        "# cargar red neuronal en pase a parametros conocidos\n",
        "\n",
        "test_data_path = \"data/test_set\"\n",
        "test_input = torch.from_numpy(numpy.loadtxt(test_data_path+\"/hechizo-7/019.txt\")).view(1, 4096)\n",
        "red_neuronal = FFNN(0, [], [sig, sig], 10, params)\n",
        "red_neuronal.forward(test_input)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "Columns 0 to 5 \n",
              " 4.7710e-02  3.9716e-05  6.6476e-04  8.2011e-10  4.3855e-05  5.6656e-06\n",
              "\n",
              "Columns 6 to 9 \n",
              " 3.6484e-06  1.4471e-04  9.5139e-01  3.8229e-08\n",
              "[torch.cuda.DoubleTensor of size 1x10 (GPU 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "Obsa_DTEq2lk",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "5af87ff7-22e6-4f15-b3bd-59999653857f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524322656795,
          "user_tz": 180,
          "elapsed": 1020,
          "user": {
            "displayName": "Martín Cornejo-Saavedra",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100137397923643336617"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "test_input_err_1 = torch.from_numpy(numpy.loadtxt(test_data_path+\"/hechizo-9/019.txt\")).double().cuda().view(64, 64)\n",
        "plt.imshow(test_input_err_1)\n",
        "print(\"El siguiente hechizo pertenece a la clase 9 pero fue clasificado como clase 1:\")\n",
        "plt.show()\n",
        "test_input_err_2 = torch.from_numpy(numpy.loadtxt(test_data_path+\"/hechizo-7/019.txt\")).double().cuda().view(64, 64)\n",
        "plt.imshow(test_input_err_2)\n",
        "print(\"El siguiente hechizo pertenece a la clase 7 pero fue clasificado como clase 8:\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El siguiente hechizo pertenece a la clase 9 pero fue clasificado como clase 1:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFMCAYAAABCsp4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0536487438>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "El siguiente hechizo pertenece a la clase 7 pero fue clasificado como clase 8:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFMCAYAAABCsp4mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f05362f9780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7KdiE3RDwERd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Checkeo de gradiente"
      ]
    },
    {
      "metadata": {
        "id": "7zcMfV-XwMb9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 4b) Chequeo de gradiente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oG-I7Oyl-qsd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Descenso de gradiente, momentum, RMSProp y Adam\n"
      ]
    },
    {
      "metadata": {
        "id": "TUkJA9Qm2GKY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 5a) Descenso de gradiente\n",
        "class SGD():\n",
        "  def __init__(self, red, lr):\n",
        "    self.red = red\n",
        "    self.lr = lr\n",
        "  \n",
        "  def step(self):\n",
        "    self.red.actualizarParams(self.lr)\n",
        "    \n",
        "#### Parte 7b) Descenso de gradiente con momentum\n",
        "class SGD2():\n",
        "  def __init__(self, red, lr, momentum=0.9):\n",
        "    pass\n",
        "  \n",
        "  def step():\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QfT8N2KY9NLI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 7c) RMSProp\n",
        "class RMSProp():\n",
        "  def __init__(self, red, lr=0.001, beta=0.9, epsilon=1e-8):\n",
        "    pass\n",
        "  \n",
        "  def step():\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Snfzj9U89XJj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 7d) Adam\n",
        "class Adam():\n",
        "  def __init__(self, red, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "    pass\n",
        "  \n",
        "  def step():\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vpCfOvvbLdoq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Datos random para carga\n"
      ]
    },
    {
      "metadata": {
        "id": "U6o4TjFZa3gJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 5b) Datos para carga\n",
        "class RandomDataset():\n",
        "  def __init__(self, N, F, C):\n",
        "    x = torch.rand(N, F)\n",
        "    self.x = torch.bernoulli(x).double().cuda()\n",
        "    self.y = torch.from_numpy(numpy.eye(C)[numpy.random.choice(C, N)]).double().cuda()\n",
        "    self.large = N\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.large\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    return (self.x[i,:], self.y[i,:])\n",
        "  \n",
        "  def paquetes(self, B):\n",
        "       \n",
        "    if not hasattr(self, 'arr_paquetes'):\n",
        "      n_iters = int(self.x.size(0)/B)\n",
        "      arr_paquetes = []  \n",
        "   \n",
        "      for index in range(n_iters):\n",
        "        arr_paquetes.append(self.elige_batch(self.x,self.y,B))\n",
        "              \n",
        "      self.arr_paquetes = arr_paquetes\n",
        "      \n",
        "    return self.arr_paquetes\n",
        "    \n",
        "  # Para elegir el siguiente batch (uno al azar) desde los datos de entrada\n",
        "  def elige_batch(self, X, Y, b):\n",
        "    N = X.size()[0]\n",
        "    x_lista = []\n",
        "    y_lista = []\n",
        "  \n",
        "    for _ in range(b):\n",
        "      i = numpy.random.randint(N)\n",
        "      x_lista.append(X[i:i+1])\n",
        "      y_lista.append(Y[i:i+1])      \n",
        "  \n",
        "    x = torch.cat(x_lista, dim=0)\n",
        "    y = torch.cat(y_lista, dim=0)\n",
        "    #pdb.set_trace()\n",
        "  \n",
        "    return x,y\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccVnn9kLS1Rl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loop de entrenamiento"
      ]
    },
    {
      "metadata": {
        "id": "Y5MNS3x0Sk9i",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 5c) Entrenando la red\n",
        "def entrenar_FFNN(red_neuronal, dataset, optimizador, epochs, B):\n",
        "  \n",
        "  perdidas = []\n",
        "  \n",
        "  for e in range(1,epochs+1):\n",
        "    for x,y in dataset.paquetes(B):\n",
        "      y_pred = red_neuronal.forward(x)\n",
        "      #pdb.set_trace()\n",
        "      loss = cross_ent_loss(y_pred,y)\n",
        "      \n",
        "      #6a) Regularización por penalización de norma\n",
        "      if (red_neuronal.wc_par != None):\n",
        "        loss += penalizacionNorma(red_neuronal.parametros, red_neuronal.wc_par, B)\n",
        "            \n",
        "      perdidas.append(loss)\n",
        "      \n",
        "      red_neuronal.backward(x, y, y_pred)\n",
        "      optimizador.step()  \n",
        "    \n",
        "  return red_neuronal, perdidas  \n",
        "\n",
        "def penalizacionNorma(arrayMatrix, alfa, B):\n",
        "  sum = 0\n",
        "  \n",
        "  for param in arrayMatrix:\n",
        "    sum += param[0].pow(2).sum()\n",
        "  \n",
        "  return sum * alfa / (2*B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4u53x0fETEq2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entrenando con datos random y graficando la pérdida"
      ]
    },
    {
      "metadata": {
        "id": "s4jgU-fDUPUD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "9a786a11-0032-4e72-9d2e-33e44f0d4b69",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524362433270,
          "user_tz": 180,
          "elapsed": 13111,
          "user": {
            "displayName": "Martín Cornejo-Saavedra",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100137397923643336617"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 5d) Graficando la pérdida en el tiempo\n",
        "\n",
        "features = 10\n",
        "clases = 3\n",
        "dataset = RandomDataset(1000,features,clases)\n",
        "\n",
        "weight_decay = 1\n",
        "\n",
        "redes = [\n",
        "    #FFNN(features, [5, 5, 5, 5], [sig, sig, sig, sig], clases, [], weight_decay),\n",
        "    FFNN(features, [15, 15], [sig, sig], clases, [], weight_decay, [1, 0.5, 0.7]),\n",
        "    #FFNN(features, [50], [sig], clases)\n",
        "]\n",
        "\n",
        "for idx, red_neuronal in enumerate(redes):\n",
        "  \n",
        "  optimizador = SGD(red_neuronal, 0.001) \n",
        "\n",
        "  epochs = 30\n",
        "  batch = 10\n",
        "  red_neuronal, perdidas = entrenar_FFNN(red_neuronal, dataset, optimizador, epochs, batch)\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(numpy.linspace(1, len(perdidas), len(perdidas)), perdidas, 'g.')\n",
        "  plt.title(\"Funcion de costo vs iteraciones: Red \"+str(idx+1))\n",
        "  plt.xlabel(\"Iteracion\")\n",
        "  plt.ylabel(\"Perdida\")\n",
        "  #pdb.set_trace()\n",
        "  \n",
        "True"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFnCAYAAABdOssgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzt+h4QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd6b4912be0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "hWkGbQPHmaFN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Entrenando con datos de varita mágica\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "7PJ_w-4UMeU9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 5e) Entrenando con datos no random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VTjXiZcTHZgm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cargando datos de MNIST"
      ]
    },
    {
      "metadata": {
        "id": "YE1iMUNfIT1r",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 8a) Cargando y visualizando datos de MNIST\n",
        "\n",
        "# Importamos las clases necesarias\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from matplotlib.pyplot import imshow, figure, subplots\n",
        "\n",
        "# Descarga y almacena el conjunto de entrenamiento y prueba de MNIST\n",
        "# Además aplica una transformación para convertir todas las imágenes a \n",
        "# tensores de pytroch\n",
        "\n",
        "train_data = MNIST('mnist', train=True, download=True, transform=ToTensor())\n",
        "test_data = MNIST('mnist', train=False, transform=ToTensor())\n",
        "\n",
        "print('Cantidad de ejemplos de entrenamiento: ' + str(len(train_data)))\n",
        "print('Cantidad de ejemplos de prueba: ' + str(len(test_data)))\n",
        "\n",
        "# Muestra 3 ejemplos al azar usando un DataLoader\n",
        "\n",
        "dataloader = DataLoader(train_data, shuffle=True)\n",
        "n_ejemplos = 3\n",
        "\n",
        "fig, axs = subplots(nrows=n_ejemplos, sharey=True, figsize=(3,n_ejemplos*3))\n",
        "\n",
        "for i, batch in enumerate(dataloader):\n",
        "  if i == n_ejemplos:\n",
        "    break\n",
        "    \n",
        "  img, d = batch\n",
        "  axs[i].set_title(\"target: \" + str(d.numpy()))\n",
        "  axs[i].imshow(img.view(28,28).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ob9VdT3NKEBP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Red neuronal para MNIST"
      ]
    },
    {
      "metadata": {
        "id": "0qWEfhdhUM_P",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 8b) Red neuronal para MNIST"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bjpfhhL9URCy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#### Parte 8c) Visualización de entrenamiento y convergencia"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JF7GV3X4WRSY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Apéndice: partes a mano"
      ]
    },
    {
      "metadata": {
        "id": "B5nWf6E7WbHf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parte 1b) Derivando las funciones de activación"
      ]
    },
    {
      "metadata": {
        "id": "quUwCy0nWhKp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\ \\text{relu}(x)}{\\partial x} =\n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t1  & \\mbox{si } x \\geq 0 \\\\\n",
        "\t\t0  & \\mbox{~} \n",
        "\t\\end{array}\n",
        "\\right. \n",
        "\\end{equation}\n",
        "<br>\n",
        "\n",
        "Dado $ \\sigma (x) = sigmoid(x)$, tenemos que:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial\\ \\text{swish}(x, \\beta)}{\\partial x} & = \\sigma (\\beta x) + \\beta x \\cdot \\sigma (\\beta x)(1-\\sigma (\\beta x)) \\\\\n",
        "& = \\sigma (\\beta x) + \\beta x \\cdot \\sigma (\\beta x) - \\beta x \\cdot \\sigma (\\beta x)^{2}  \\\\\n",
        "&= \\beta \\cdot swish(x, \\beta) + \\sigma (\\beta x)(1 - \\beta \\cdot swish(x, \\beta))\\\\\n",
        "\\\\\n",
        "\\frac{\\partial\\ \\text{swish}(x, \\beta)}{\\partial \\beta} & =  \n",
        "x^2 \\sigma (\\beta x)(1 - \\sigma (\\beta x))\\\\\n",
        "\\end{eqnarray}\n",
        "<br><br>\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial x} & =  \n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t1  & \\mbox{si } x \\geq 0 \\\\\n",
        "\t\texp (\\frac{x}{\\alpha})  & \\mbox{~} \n",
        "\t\\end{array}\n",
        "\\right. \\\\\n",
        "\\\\\n",
        "\\frac{\\partial\\ \\text{celu}(x, \\alpha)}{\\partial \\alpha} & = \n",
        "\\left\\{\n",
        "\t\\begin{array}{ll}\n",
        "\t\t0  & \\mbox{si } x \\geq 0 \\\\\n",
        "\t\texp (\\frac{x}{\\alpha})(1 - \\frac{x}{\\alpha}) - 1  & \\mbox{~} \n",
        "\t\\end{array}\n",
        "\\right. \\\\\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "metadata": {
        "id": "rTWKAEHkoE3K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parte 1c) Softmax"
      ]
    },
    {
      "metadata": {
        "id": "WRk-15ANoLX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Dada la funcion `softmax` sabemos que cada elemento de la secuencia $\\text{softmax}(x_1,\\ldots,x_n)$ tiene la forma\n",
        "\n",
        "\\begin{equation}\n",
        "s_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n",
        "\\end{equation}\n",
        "\n",
        "Luego, para cada elemento de la secuencia $\\text{softmax}(x_1-M,\\ldots,x_n-M)$ se tiene\n",
        "\n",
        "\\begin{equation}\n",
        "s_i = \\frac{e^{x_i-M}}{\\sum_{j=1}^{n}e^{x_j-M}} = \\frac{e^{-M}e^{x_i}}{\\sum_{j=1}^{n}e^{-M}e^{x_j}} = \\frac{e^{-M}e^{x_i}}{e^{-M}\\sum_{j=1}^{n}e^{x_j}} = \\frac{e^{x_i}}{\\sum_{j=1}^{n}e^{x_j}}\n",
        "\\end{equation}\n",
        "\n",
        "Demostrando que $\\text{softmax}(x_1-M,\\ldots,x_n-M) = \\text{softmax}(x_1,\\ldots,x_n)$."
      ]
    },
    {
      "metadata": {
        "id": "dORBkUQ4Wh1z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parte 3b) Derviando la última capa"
      ]
    },
    {
      "metadata": {
        "id": "NZlKnTVrWp8y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "\\frac{\\partial \\cal L}{\\partial u^{(L)}} = \\frac{\\partial \\cal L}{\\partial ŷ} \\cdot \\frac{\\partial ŷ}{\\partial u^{(L)}} \\\\\n",
        "\\end{equation}\n",
        "\n",
        "Usando la notación de Einstein tenemos que:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "(\\frac{\\partial\\cal L}{\\partial U})_{ij} &= \\frac{\\partial\\cal L}{\\partial u^{(L)}_{kl}} \\frac{\\partial u^{(L)}_{kl}}{\\partial U_{ij}} \\\\\n",
        "\\end{eqnarray}\n",
        "\n",
        "Luego,\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial u^{(L)}_{kl}}{\\partial U_{ij}} = \\frac{\\partial (h^{(L)}_{kr}U_{rl} + c_{l})}{\\partial U_{ij}} = \\left\\{\n",
        "    \\begin{array}{}\n",
        "\t\th^{(L)}_{ki}  & \\mbox{si } r = i,\\ l = j \\\\\n",
        "\t\t0  & \\mbox{~}\n",
        "    \\end{array}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "Entonces,\n",
        "\n",
        "\\begin{eqnarray}\n",
        "(\\frac{\\partial\\cal L}{\\partial U})_{ij} &= \\frac{\\partial\\cal L}{\\partial u^{(L)}_{kl}} h^{(L)}_{ki} = \\frac{\\partial\\cal L}{\\partial u^{(L)}_{kj}} h^{(L)}_{ki} \\\\\n",
        "\\\\\n",
        "& \\boxed{ \\frac{\\partial\\cal L}{\\partial U} = (h^{(L)})^{T} \\frac{\\partial\\cal L}{\\partial u^{(L)}} }\n",
        "\\end{eqnarray}\n",
        "\n",
        "Análogamente\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial u^{(L)}_{kl}}{\\partial c_{i}} = \\frac{\\partial (h^{(L)}_{kr}U_{rl} + c_{l})}{\\partial c_{i}} = \\left\\{\n",
        "    \\begin{array}{}\n",
        "\t\t1  & \\mbox{si } l = j \\\\\n",
        "\t\t0  & \\mbox{~}\n",
        "    \\end{array}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{eqnarray}\n",
        "(\\frac{\\partial\\cal L}{\\partial c})_{i} &= \\frac{\\partial\\cal L}{\\partial u^{(L)}_{ki}} \\cdot 1\n",
        "\\end{eqnarray}\n",
        "\n",
        "\\begin{equation}\n",
        "\\boxed{ \\frac{\\partial\\cal L}{\\partial c} = [1 \\ldots 1] \\frac{\\partial\\cal L}{\\partial u^{(L)}} }\n",
        "\\end{equation}\n",
        "\n",
        "Donde $[1 \\ldots 1]$ es un vector de unos de largo correspondiente al numero de fila del resultado de $\\frac{\\partial\\cal L}{\\partial u^{(L)}}$ \n",
        "\n",
        "Finalmente,\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial u^{(L)}_{kl}}{\\partial h^{(L)}_{ij}} = \\frac{\\partial (h^{(L)}_{kr}U_{rl} + c_{l})}{\\partial h^{(L)}_{ij}} = \\left\\{\n",
        "    \\begin{array}{}\n",
        "\t\tU_{jl}  & \\mbox{si } k = i,\\ r = j \\\\\n",
        "\t\t0  & \\mbox{~}\n",
        "    \\end{array}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{eqnarray}\n",
        "(\\frac{\\partial\\cal L}{\\partial h^{(L)}})_{ij} &= \\frac{\\partial\\cal L}{\\partial u^{(L)}_{kl}} U_{jl} = \\frac{\\partial\\cal L}{\\partial u^{(L)}_{il}} U_{lj}^{T} \\\\\n",
        "\\\\\n",
        "& \\boxed{ \\frac{\\partial\\cal L}{\\partial h^{(L)}} = \\frac{\\partial\\cal L}{\\partial u^{(L)}} U^{T} }\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "metadata": {
        "id": "hPowZjVDWrrL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parte 3c) Derivando desde las capas escondidas"
      ]
    },
    {
      "metadata": {
        "id": "WhJJTBD7Wx-s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial u^{(k)}} = \\frac{\\partial\\cal L}{\\partial h^{(k)}} \\frac{\\partial\\cal h^{(k)}}{\\partial u^{(k)}} \\\\\n",
        "\\end{equation}\n",
        "\n",
        "Para __sigmoid__ tenemos la siguiente derivada:\n",
        "\\begin{equation}\n",
        "h^{(k)} = sig(u^{(k)})\n",
        "\\\\\n",
        "\\frac{\\partial\\cal h^{(k)}}{\\partial u^{(k)}} = h^{(k)}(1 - h^{(k)})\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "Las siguientes derivadas pueden obtenerse independiente de la forma de la función de activación y son análogas a las calculadas en la última capa:\n",
        "<br><br>\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial W^{(k)}} = (h^{(k)})^{T} \\frac{\\partial\\cal L}{\\partial u^{(k)}} \\\\\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial b^{(k)}} = [1 \\ldots 1] \\frac{\\partial\\cal L}{\\partial u^{(k)}} \\\\\n",
        "\\end{equation}\n",
        "<br><br>\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial\\cal L}{\\partial h^{(k-1)}} = \\frac{\\partial\\cal L}{\\partial u^{(k)}} (W^{(k)})^T \\\\\n",
        "\\end{equation}"
      ]
    },
    {
      "metadata": {
        "id": "kwqO70d2W2if",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Otras derivadas (derivadas opcionales de celu y swish, de batch normalization, etc.)"
      ]
    },
    {
      "metadata": {
        "id": "J-VAm34vXBAs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Parte 6a) Weight Decay"
      ]
    },
    {
      "metadata": {
        "id": "SJleLECLOer_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$w_{ij_{n+1}} = (1 - \\frac{\\lambda \\alpha }{N})w_{ij_{n}} - \\lambda \\frac{\\partial \\cal L}{\\partial w_{ij_{n}}}$"
      ]
    }
  ]
}